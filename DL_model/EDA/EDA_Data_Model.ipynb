{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "train_ids = ['001', '005', '006', '009', '010', '011', '012', '013', '015', '021', '022', '023',\n",
    "             '024', '026', '028', '031', '032', '033', '036', '038', '041', '042', '044',\n",
    "             '045', '047', '053', '054', '055', '060', '063', '064', '066', '070', '072',\n",
    "             '073', '075', '076', '077', '078']\n",
    "test_ids = ['002', '003', '007', '017', '018', '019', '020', '025', '029', '030', '039', '043',\n",
    "            '048', '052', '059', '065', '068', '074']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', 'disgust', 'fear', 'sad', 'happy', 'anger', 'neutral', 'anger', 'disgust', 'happy', 'sad', 'fear', 'fear', 'happy', 'sad', 'neutral', 'disgust', 'anger', 'neutral', 'anger', 'sad', 'disgust', 'fear', 'happy', 'fear', 'happy', 'sad', 'neutral', 'anger', 'disgust', 'fear', 'sad', 'anger', 'neutral', 'happy', 'disgust', 'happy', 'disgust', 'fear', 'anger', 'sad', 'neutral', 'anger', 'neutral', 'fear', 'sad', 'disgust', 'happy', 'happy', 'neutral', 'anger', 'fear', 'sad', 'disgust', 'anger', 'fear', 'neutral', 'disgust', 'sad', 'happy', 'disgust', 'sad', 'neutral', 'happy', 'fear', 'anger', 'sad', 'fear', 'neutral', 'happy', 'disgust', 'anger', 'fear', 'sad', 'disgust', 'happy', 'neutral', 'anger', 'neutral', 'disgust', 'anger', 'happy', 'sad', 'fear', 'fear', 'neutral', 'disgust', 'anger', 'sad', 'happy', 'sad', 'happy', 'disgust', 'anger', 'fear', 'neutral', 'happy', 'neutral', 'disgust', 'sad', 'fear', 'anger', 'happy', 'sad', 'neutral', 'fear', 'disgust', 'anger', 'happy', 'anger', 'sad', 'fear', 'disgust', 'neutral', 'happy', 'neutral', 'fear', 'sad', 'anger', 'disgust', 'anger', 'sad', 'neutral', 'happy', 'disgust', 'fear', 'sad', 'neutral', 'anger', 'fear', 'disgust', 'happy', 'happy', 'neutral', 'anger', 'sad', 'disgust', 'fear', 'neutral', 'sad', 'disgust', 'happy', 'fear', 'anger', 'disgust', 'anger', 'fear', 'neutral', 'happy', 'sad', 'neutral', 'happy', 'sad', 'fear', 'disgust', 'anger', 'fear', 'anger', 'disgust', 'happy', 'neutral', 'sad', 'sad', 'anger', 'neutral', 'disgust', 'happy', 'fear', 'happy', 'anger', 'sad', 'disgust', 'neutral', 'fear', 'anger', 'happy', 'fear', 'disgust', 'sad', 'neutral', 'happy', 'anger', 'disgust', 'sad', 'fear', 'neutral', 'neutral', 'disgust', 'happy', 'fear', 'sad', 'anger', 'happy', 'fear', 'anger', 'sad', 'neutral', 'disgust', 'happy', 'fear', 'sad', 'neutral', 'disgust', 'anger', 'disgust', 'anger', 'neutral', 'sad', 'happy', 'fear', 'sad', 'disgust', 'anger', 'neutral', 'fear', 'happy', 'happy', 'sad', 'disgust', 'fear', 'neutral', 'anger', 'fear', 'anger', 'happy', 'sad', 'neutral', 'disgust', 'neutral', 'disgust', 'anger', 'sad', 'happy', 'fear']\n"
     ]
    }
   ],
   "source": [
    "video_id_file = \"video_id.csv\"\n",
    "video_id_df = pd.read_csv(video_id_file)\n",
    "# 두 번째 열을 'emotion' 열로 대체\n",
    "video_id_df = video_id_df.rename(columns={video_id_df.columns[1]: 'emotion'})\n",
    "emotions = video_id_df['emotion'].tolist()\n",
    "print(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13690\n"
     ]
    }
   ],
   "source": [
    "def get_max_sequence_length(ids):\n",
    "    max_len = 0\n",
    "    for id in ids:\n",
    "        video_id_file = \"video_id.csv\"\n",
    "        video_id_df = pd.read_csv(video_id_file)\n",
    "        video_id_df['video_id'] = video_id_df['video_id'].apply(lambda x: str(x).zfill(3))\n",
    "        video_id_df = video_id_df.rename(columns={video_id_df.columns[1]: 'emotion'})\n",
    "        emotions = video_id_df[video_id_df.video_id == id]['emotion'].tolist()\n",
    "        for emotion in emotions:\n",
    "            file_path = f\"{id}/{id}_{emotion}.csv\"\n",
    "            data = pd.read_csv(file_path, header=None)\n",
    "            sequence_length = len(data)\n",
    "            if sequence_length > max_len:\n",
    "                max_len = sequence_length\n",
    "    return max_len \n",
    "\n",
    "# 최대 시퀀스 길이 확인\n",
    "max_sequence_length = get_max_sequence_length(train_ids)\n",
    "print(max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(ids, max_len=max_sequence_length):\n",
    "    features = []\n",
    "    scaler = StandardScaler()\n",
    "    for id in ids:\n",
    "        # video_id.csv 파일 가져오기, id 형식 변환\n",
    "        video_id_file = \"video_id.csv\"\n",
    "        video_id_df = pd.read_csv(video_id_file)\n",
    "        video_id_df['video_id'] = video_id_df['video_id'].apply(lambda x: str(x).zfill(3))\n",
    "        # 두 번째 열을 'emotion' 열로 대체\n",
    "        video_id_df = video_id_df.rename(columns={video_id_df.columns[1]: 'emotion'})\n",
    "        # 해당 id의 감정 순서 리스트 생성\n",
    "        emotions = video_id_df[video_id_df.video_id == id]['emotion'].tolist()\n",
    "        for emotion in emotions:\n",
    "            # 감정에 해당하는 파일 경로 탐색\n",
    "            file_path = f\"{id}/{id}_{emotion}.csv\"\n",
    "            data = pd.read_csv(file_path, header=None)\n",
    "            # 데이터 2D 배열로 변환\n",
    "            data = data.values.reshape(-1, 1)\n",
    "            # 데이터 표준화\n",
    "            data = scaler.fit_transform(data)\n",
    "            # 데이터를 max_len으로 잘라냄\n",
    "            data = data[:max_len]\n",
    "            # 만약 데이터의 길이가 max_len보다 작다면, 나머지 부분을 패딩\n",
    "            if len(data) < max_len:\n",
    "                data = np.pad(data, ((0, max_len - len(data)), (0, 0)), 'constant')\n",
    "            # 데이터 추가\n",
    "            features.append(data)\n",
    "\n",
    "    features_padded = np.array(features)\n",
    "    return features_padded\n",
    "\n",
    "train_features = load_and_preprocess_data(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(234, 13690, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2.93475448]\n",
      "  [ 2.99404729]\n",
      "  [ 2.96438931]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[-0.50565603]\n",
      "  [-0.51482795]\n",
      "  [-0.51482795]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 2.53794074]\n",
      "  [ 2.57023986]\n",
      "  [ 2.57023986]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.16702695]\n",
      "  [-1.16702695]\n",
      "  [ 0.06997831]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 2.6772027 ]\n",
      "  [ 0.17921862]\n",
      "  [ 0.17921862]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[-0.56540335]\n",
      "  [-1.82864931]\n",
      "  [-0.56540335]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_shape, encoding_dim):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Flatten layer: the Dense layer works on 2D inputs.\n",
    "    flat_layer = Flatten()(input_layer)\n",
    "\n",
    "    # Encoding layer\n",
    "    encoded = Dense(encoding_dim, activation='relu')(flat_layer)\n",
    "\n",
    "    # Decoding layer\n",
    "    decoded = Dense(np.prod(input_shape), activation='sigmoid')(encoded)\n",
    "\n",
    "    # Reshape layer: to match the original input shape.\n",
    "    reshaped = Reshape(input_shape)(decoded)\n",
    "\n",
    "    # Autoencoder model\n",
    "    autoencoder = Model(input_layer, reshaped)\n",
    "    \n",
    "    # Encoder model\n",
    "    encoder = Model(input_layer, encoded)\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Encoding dimension\n",
    "encoding_dim = 64  # or any other value\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder, encoder = create_autoencoder((max_sequence_length, 1), encoding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 1s 73ms/step - loss: 0.3968\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3854\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3526\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3003\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.2465\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.2059\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.1814\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.1681\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.1606\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.1555\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.1523\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.1498\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.1478\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.1464\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.1452\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1442\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1435\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1428\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.1421\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.1416\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1411\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1407\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.1402\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.1398\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.1394\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.1391\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.1387\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.1384\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.1381\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.1377\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.1375\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1372\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1369\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.1366\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1363\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1360\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1357\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1355\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.1352\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.1350\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.1347\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.1345\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.1342\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.1340\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.1338\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1336\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.1333\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.1331\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1329\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.1327\n",
      "8/8 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "autoencoder.fit(train_features, train_features, epochs=50, batch_size=128, shuffle=True)\n",
    "\n",
    "# Extracting features\n",
    "train_features_encoded = encoder.predict(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(234, 64)\n"
     ]
    }
   ],
   "source": [
    "print(train_features_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.550317,  0.      , 21.81655 , ..., 21.984102, 21.86208 ,\n",
       "        13.78929 ],\n",
       "       [ 0.      , 14.353313,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [14.540529,  0.      , 24.586775, ..., 24.902727, 23.910677,\n",
       "        16.990757],\n",
       "       ...,\n",
       "       [ 0.      , 33.022785,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [41.18801 ,  0.      ,  0.      , ...,  0.      , 22.506584,\n",
       "         0.      ],\n",
       "       [ 0.      , 19.07907 , 37.35222 , ..., 30.521257, 30.937357,\n",
       "         0.      ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encoded features to a DataFrame\n",
    "df_encoded = pd.DataFrame(train_features_encoded)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_file = \"encoded_features_EDA.csv\"\n",
    "df_encoded.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'neutral', 'happy', 'sad', 'fear', 'happy', 'neutral', 'anger', 'sad', 'disgust', 'fear', 'disgust', 'sad', 'happy', 'fear', 'neutral', 'anger', 'disgust', 'neutral', 'fear', 'anger', 'sad', 'happy', 'anger', 'fear', 'sad', 'disgust', 'neutral', 'happy', 'anger', 'disgust', 'neutral', 'happy', 'sad', 'fear', 'happy', 'fear', 'anger', 'neutral', 'disgust', 'sad', 'fear', 'happy', 'sad', 'neutral', 'disgust', 'anger', 'anger', 'disgust', 'sad', 'happy', 'fear', 'neutral', 'disgust', 'anger', 'neutral', 'fear', 'sad', 'happy', 'neutral', 'disgust', 'happy', 'sad', 'anger', 'fear', 'anger', 'sad', 'fear', 'happy', 'neutral', 'disgust', 'disgust', 'neutral', 'fear', 'happy', 'sad', 'anger', 'sad', 'neutral', 'anger', 'disgust', 'fear', 'happy', 'disgust', 'fear', 'sad', 'neutral', 'anger', 'happy', 'neutral', 'happy', 'fear', 'sad', 'disgust', 'anger', 'neutral', 'happy', 'anger', 'fear', 'sad', 'disgust', 'neutral', 'anger', 'happy', 'disgust', 'sad', 'fear']\n"
     ]
    }
   ],
   "source": [
    "video_id_file = \"video_id_test.csv\"\n",
    "video_id_df = pd.read_csv(video_id_file)\n",
    "# 두 번째 열을 'emotion' 열로 대체\n",
    "video_id_df = video_id_df.rename(columns={video_id_df.columns[1]: 'emotion'})\n",
    "emotions = video_id_df['emotion'].tolist()\n",
    "print(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27226\n"
     ]
    }
   ],
   "source": [
    "def get_max_sequence_length(ids):\n",
    "    max_len = 0\n",
    "    for id in ids:\n",
    "        video_id_file = \"video_id_test.csv\"\n",
    "        video_id_df = pd.read_csv(video_id_file)\n",
    "        video_id_df['video_id'] = video_id_df['video_id'].apply(lambda x: str(x).zfill(3))\n",
    "        video_id_df = video_id_df.rename(columns={video_id_df.columns[1]: 'emotion'})\n",
    "        emotions = video_id_df[video_id_df.video_id == id]['emotion'].tolist()\n",
    "        for emotion in emotions:\n",
    "            file_path = f\"{id}/{id}_{emotion}.csv\"\n",
    "            data = pd.read_csv(file_path, header=None)\n",
    "            sequence_length = len(data)\n",
    "            if sequence_length > max_len:\n",
    "                max_len = sequence_length\n",
    "    return max_len \n",
    "\n",
    "# 최대 시퀀스 길이 확인\n",
    "max_sequence_length_test = get_max_sequence_length(test_ids)\n",
    "print(max_sequence_length_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(ids, max_len=max_sequence_length_test):\n",
    "    features = []\n",
    "    scaler = StandardScaler()\n",
    "    for id in ids:\n",
    "        # video_id_test.csv 파일 가져오기, id 형식 변환\n",
    "        video_id_file = \"video_id_test.csv\"\n",
    "        video_id_df = pd.read_csv(video_id_file)\n",
    "        video_id_df['video_id'] = video_id_df['video_id'].apply(lambda x: str(x).zfill(3))\n",
    "        # 두 번째 열을 'emotion' 열로 대체\n",
    "        video_id_df = video_id_df.rename(columns={video_id_df.columns[1]: 'emotion'})\n",
    "        # 해당 id의 감정 순서 리스트 생성\n",
    "        emotions = video_id_df[video_id_df.video_id == id]['emotion'].tolist()\n",
    "        for emotion in emotions:\n",
    "            # 감정에 해당하는 파일 경로 탐색\n",
    "            file_path = f\"{id}/{id}_{emotion}.csv\"\n",
    "            data = pd.read_csv(file_path, header=None)\n",
    "            # 데이터 2D 배열로 변환\n",
    "            data = data.values.reshape(-1, 1)\n",
    "            # 데이터 표준화\n",
    "            data = scaler.fit_transform(data)\n",
    "            # 데이터를 max_len으로 잘라냄\n",
    "            data = data[:max_len]\n",
    "            # 만약 데이터의 길이가 max_len보다 작다면, 나머지 부분을 패딩\n",
    "            if len(data) < max_len:\n",
    "                data = np.pad(data, ((0, max_len - len(data)), (0, 0)), 'constant')\n",
    "            # 데이터 추가\n",
    "            features.append(data)\n",
    "\n",
    "    features_padded = np.array(features)\n",
    "    return features_padded\n",
    "\n",
    "test_features = load_and_preprocess_data(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 27226, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.13545188]\n",
      "  [-1.13545188]\n",
      "  [-1.13545188]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.16463266]\n",
      "  [-1.14347553]\n",
      "  [-1.14347553]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 1.55791807]\n",
      "  [ 1.56399168]\n",
      "  [ 1.56196609]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.25330206]\n",
      "  [ 0.25198198]\n",
      "  [ 0.24802071]\n",
      "  ...\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_shape, encoding_dim):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Flatten layer: the Dense layer works on 2D inputs.\n",
    "    flat_layer = Flatten()(input_layer)\n",
    "\n",
    "    # Encoding layer\n",
    "    encoded = Dense(encoding_dim, activation='relu')(flat_layer)\n",
    "\n",
    "    # Decoding layer\n",
    "    decoded = Dense(np.prod(input_shape), activation='sigmoid')(encoded)\n",
    "\n",
    "    # Reshape layer: to match the original input shape.\n",
    "    reshaped = Reshape(input_shape)(decoded)\n",
    "\n",
    "    # Autoencoder model\n",
    "    autoencoder = Model(input_layer, reshaped)\n",
    "    \n",
    "    # Encoder model\n",
    "    encoder = Model(input_layer, encoded)\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Encoding dimension\n",
    "encoding_dim = 64  # or any other value\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder, encoder = create_autoencoder((max_sequence_length_test, 1), encoding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3549\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.3538\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.3500\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.3406\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.3240\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.3010\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.2738\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.2452\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.2179\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.1937\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.1740\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.1587\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.1474\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.1392\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.1331\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.1286\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.1252\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.1227\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.1207\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.1192\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.1179\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.1169\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.1161\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.1155\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.1149\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.1144\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.1140\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.1137\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1133\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1130\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.1127\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.1125\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1122\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.1119\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1117\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1114\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.1112\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.1110\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1108\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1106\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.1104\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.1102\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1100\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.1099\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.1097\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.1095\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.1094\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.1093\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.1091\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.1090\n",
      "4/4 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "autoencoder.fit(test_features, test_features, epochs=50, batch_size=128, shuffle=True)\n",
    "\n",
    "# Extracting features\n",
    "test_features_encoded = encoder.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 64)\n"
     ]
    }
   ],
   "source": [
    "print(test_features_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 1.2783803e+00, 3.2280754e+01, ..., 3.9714165e+01,\n",
       "        5.2484436e+01, 3.9255604e+01],\n",
       "       [4.6772931e-02, 4.7429938e-02, 4.7759414e-02, ..., 4.9848713e-02,\n",
       "        4.9521651e-02, 4.9831320e-02],\n",
       "       [4.6772931e-02, 4.7429938e-02, 4.7759414e-02, ..., 4.9848713e-02,\n",
       "        4.9521651e-02, 4.9831320e-02],\n",
       "       ...,\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 1.7787821e+01,\n",
       "        0.0000000e+00, 1.7016611e+01],\n",
       "       [1.1985534e+01, 2.4901291e+01, 2.6198778e+01, ..., 0.0000000e+00,\n",
       "        2.3725740e+01, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 2.3570986e+01,\n",
       "        0.0000000e+00, 2.3624538e+01]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encoded features to a DataFrame\n",
    "df_encoded = pd.DataFrame(test_features_encoded)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_file = \"encoded_features_test_EDA.csv\"\n",
    "df_encoded.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
