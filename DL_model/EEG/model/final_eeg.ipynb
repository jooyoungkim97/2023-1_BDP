{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d34cf6",
   "metadata": {},
   "source": [
    "## EEG data grouing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ba68b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 경로 설정\n",
    "eeg_folder_path = '/Users/sh_oh/Library/CloudStorage/Dropbox/Data/2023-1/BDP/ECSMP_Dataset/EEG_train_csv'\n",
    "\n",
    "# 파일 목록 가져오기\n",
    "file_list = os.listdir(eeg_folder_path)\n",
    "\n",
    "# 그룹별로 파일들을 그룹화\n",
    "grouped_files = {}\n",
    "for file_name in file_list:\n",
    "    group_key = file_name[:11]\n",
    "    if group_key == \".DS_Store\":  # .DS_Store 그룹은 건너뜁니다\n",
    "        continue\n",
    "    if group_key not in grouped_files:\n",
    "        grouped_files[group_key] = []\n",
    "    grouped_files[group_key].append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c27d8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 데이터를 저장할 리스트\n",
    "new_data = []\n",
    "\n",
    "# 그룹별로 데이터 처리\n",
    "for key, file_names in grouped_files.items():\n",
    "    # 그룹 내 파일들을 읽어와 데이터 리스트에 추가\n",
    "    group_data = []\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(eeg_folder_path, file_name)\n",
    "        df = pd.read_csv(file_path, encoding='latin1')  # 인코딩 변경\n",
    "        channel_data = df.iloc[:, 0].values  # 첫 번째 열의 데이터만 사용\n",
    "        group_data.append(channel_data)\n",
    "    \n",
    "    # 그룹 데이터를 평균하여 새로운 데이터 생성\n",
    "    new_group_data = np.mean(group_data, axis=0)\n",
    "    new_data.append(new_group_data)\n",
    "\n",
    "# 저장할 폴더 경로\n",
    "output_folder_path = '/Users/sh_oh/Library/CloudStorage/Dropbox/Data/2023-1/BDP/ECSMP_Dataset/EEG_train2_csv'\n",
    "\n",
    "# 데이터를 CSV 파일로 저장\n",
    "for i, group_data in enumerate(new_data):\n",
    "    file_name = f\"group_{i+1}.csv\"\n",
    "    file_path = os.path.join(output_folder_path, file_name)\n",
    "    pd.DataFrame(group_data).to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "78377271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그룹 개수: 234\n",
      "001_video_1\n",
      "001_video_2\n",
      "001_video_3\n",
      "001_video_4\n",
      "001_video_5\n",
      "001_video_6\n",
      "005_video_1\n",
      "005_video_2\n",
      "005_video_3\n",
      "005_video_4\n",
      "005_video_5\n",
      "005_video_6\n",
      "006_video_1\n",
      "006_video_2\n",
      "006_video_3\n",
      "006_video_4\n",
      "006_video_5\n",
      "006_video_6\n",
      "009_video_1\n",
      "009_video_2\n",
      "009_video_3\n",
      "009_video_4\n",
      "009_video_5\n",
      "009_video_6\n",
      "010_video_1\n",
      "010_video_2\n",
      "010_video_3\n",
      "010_video_4\n",
      "010_video_5\n",
      "010_video_6\n",
      "011_video_1\n",
      "011_video_2\n",
      "011_video_3\n",
      "011_video_4\n",
      "011_video_5\n",
      "011_video_6\n",
      "012_video_1\n",
      "012_video_2\n",
      "012_video_3\n",
      "012_video_4\n",
      "012_video_5\n",
      "012_video_6\n",
      "013_video_1\n",
      "013_video_2\n",
      "013_video_3\n",
      "013_video_4\n",
      "013_video_5\n",
      "013_video_6\n",
      "015_video_1\n",
      "015_video_2\n",
      "015_video_3\n",
      "015_video_4\n",
      "015_video_5\n",
      "015_video_6\n",
      "021_video_1\n",
      "021_video_2\n",
      "021_video_3\n",
      "021_video_4\n",
      "021_video_5\n",
      "021_video_6\n",
      "022_video_1\n",
      "022_video_2\n",
      "022_video_3\n",
      "022_video_4\n",
      "022_video_5\n",
      "022_video_6\n",
      "023_video_1\n",
      "023_video_2\n",
      "023_video_3\n",
      "023_video_4\n",
      "023_video_5\n",
      "023_video_6\n",
      "024_video_1\n",
      "024_video_2\n",
      "024_video_3\n",
      "024_video_4\n",
      "024_video_5\n",
      "024_video_6\n",
      "026_video_1\n",
      "026_video_2\n",
      "026_video_3\n",
      "026_video_4\n",
      "026_video_5\n",
      "026_video_6\n",
      "028_video_1\n",
      "028_video_2\n",
      "028_video_3\n",
      "028_video_4\n",
      "028_video_5\n",
      "028_video_6\n",
      "031_video_1\n",
      "031_video_2\n",
      "031_video_3\n",
      "031_video_4\n",
      "031_video_5\n",
      "031_video_6\n",
      "032_video_1\n",
      "032_video_2\n",
      "032_video_3\n",
      "032_video_4\n",
      "032_video_5\n",
      "032_video_6\n",
      "033_video_1\n",
      "033_video_2\n",
      "033_video_3\n",
      "033_video_4\n",
      "033_video_5\n",
      "033_video_6\n",
      "036_video_1\n",
      "036_video_2\n",
      "036_video_3\n",
      "036_video_4\n",
      "036_video_5\n",
      "036_video_6\n",
      "038_video_1\n",
      "038_video_2\n",
      "038_video_3\n",
      "038_video_4\n",
      "038_video_5\n",
      "038_video_6\n",
      "041_video_1\n",
      "041_video_2\n",
      "041_video_3\n",
      "041_video_4\n",
      "041_video_5\n",
      "041_video_6\n",
      "042_video_1\n",
      "042_video_2\n",
      "042_video_3\n",
      "042_video_4\n",
      "042_video_5\n",
      "042_video_6\n",
      "044_video_1\n",
      "044_video_2\n",
      "044_video_3\n",
      "044_video_4\n",
      "044_video_5\n",
      "044_video_6\n",
      "045_video_1\n",
      "045_video_2\n",
      "045_video_3\n",
      "045_video_4\n",
      "045_video_5\n",
      "045_video_6\n",
      "047_video_1\n",
      "047_video_2\n",
      "047_video_3\n",
      "047_video_4\n",
      "047_video_5\n",
      "047_video_6\n",
      "053_video_1\n",
      "053_video_2\n",
      "053_video_3\n",
      "053_video_4\n",
      "053_video_5\n",
      "053_video_6\n",
      "054_video_1\n",
      "054_video_2\n",
      "054_video_3\n",
      "054_video_4\n",
      "054_video_5\n",
      "054_video_6\n",
      "055_video_1\n",
      "055_video_2\n",
      "055_video_3\n",
      "055_video_4\n",
      "055_video_5\n",
      "055_video_6\n",
      "060_video_1\n",
      "060_video_2\n",
      "060_video_3\n",
      "060_video_4\n",
      "060_video_5\n",
      "060_video_6\n",
      "063_video_1\n",
      "063_video_2\n",
      "063_video_3\n",
      "063_video_4\n",
      "063_video_5\n",
      "063_video_6\n",
      "064_video_1\n",
      "064_video_2\n",
      "064_video_3\n",
      "064_video_4\n",
      "064_video_5\n",
      "064_video_6\n",
      "066_video_1\n",
      "066_video_2\n",
      "066_video_3\n",
      "066_video_4\n",
      "066_video_5\n",
      "066_video_6\n",
      "070_video_1\n",
      "070_video_2\n",
      "070_video_3\n",
      "070_video_4\n",
      "070_video_5\n",
      "070_video_6\n",
      "072_video_1\n",
      "072_video_2\n",
      "072_video_3\n",
      "072_video_4\n",
      "072_video_5\n",
      "072_video_6\n",
      "073_video_1\n",
      "073_video_2\n",
      "073_video_3\n",
      "073_video_4\n",
      "073_video_5\n",
      "073_video_6\n",
      "075_video_1\n",
      "075_video_2\n",
      "075_video_3\n",
      "075_video_4\n",
      "075_video_5\n",
      "075_video_6\n",
      "076_video_1\n",
      "076_video_2\n",
      "076_video_3\n",
      "076_video_4\n",
      "076_video_5\n",
      "076_video_6\n",
      "077_video_1\n",
      "077_video_2\n",
      "077_video_3\n",
      "077_video_4\n",
      "077_video_5\n",
      "077_video_6\n",
      "078_video_1\n",
      "078_video_2\n",
      "078_video_3\n",
      "078_video_4\n",
      "078_video_5\n",
      "078_video_6\n"
     ]
    }
   ],
   "source": [
    "# 그룹 개수 출력\n",
    "print(f\"그룹 개수: {len(grouped_files)}\")\n",
    "\n",
    "# 그룹 이름 출력 (정렬하여)\n",
    "for group_name in sorted(grouped_files.keys()):\n",
    "    print(group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef8ff3",
   "metadata": {},
   "source": [
    "# EEG preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "662967bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 경로 설정\n",
    "eeg_folder_path = '/Users/sh_oh/Library/CloudStorage/Dropbox/Data/2023-1/BDP/ECSMP_Dataset/EEG_train2_csv'\n",
    "\n",
    "# 파일 목록 가져오기\n",
    "file_list = os.listdir(eeg_folder_path)\n",
    "\n",
    "# 전체 데이터를 저장할 리스트\n",
    "data = []\n",
    "\n",
    "# 가장 긴 데이터 길이를 기준으로 zero-padding\n",
    "max_length = 0\n",
    "\n",
    "# 데이터 읽어오기 및 전처리\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(eeg_folder_path, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    channel_data = df.iloc[:, 0].values  # 첫 번째 열의 데이터만 사용\n",
    "    data.append(channel_data)\n",
    "    if len(channel_data) > max_length:\n",
    "        max_length = len(channel_data)\n",
    "\n",
    "# Zero-padding\n",
    "padded_data = pad_sequences(data, maxlen=max_length, padding='post')\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(padded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "23e80495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type: <class 'numpy.ndarray'>\n",
      "Sample Data:\n",
      "[[0.21246246 0.214514   0.21329929 ... 1.         1.         1.        ]\n",
      " [0.21638002 0.21883954 0.21866678 ... 1.         1.         1.        ]\n",
      " [0.22294567 0.22428091 0.22318315 ... 1.         1.         1.        ]\n",
      " [0.35944581 0.35381309 0.35068501 ... 1.         1.         1.        ]\n",
      " [0.8462872  0.85426769 0.8569997  ... 1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 타입 확인\n",
    "print(\"Data Type:\", type(normalized_data))\n",
    "\n",
    "# 데이터 일부 출력\n",
    "print(\"Sample Data:\")\n",
    "print(normalized_data[:5])  # 예시로 처음 2개 데이터 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbcbe8f",
   "metadata": {},
   "source": [
    "# Denoising AutoEncoder(DAE) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f46c40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# 가우시안 노이즈 추가\n",
    "def add_gaussian_noise(data, noise_factor):\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_factor, size=data.shape)\n",
    "    noisy_data = data + noise\n",
    "    return noisy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0bd37689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoising Autoencoder 모델 생성\n",
    "def create_denoising_autoencoder(input_shape, encoding_dim):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = Reshape((input_shape[0], 1))(input_layer)\n",
    "    encoded = Conv1D(128, kernel_size=3, activation='relu', padding='same')(encoded)\n",
    "    encoded = Conv1D(64, kernel_size=3, activation='relu', padding='same')(encoded)\n",
    "    encoded = Conv1D(32, kernel_size=3, activation='relu', padding='same')(encoded)\n",
    "    encoded = Conv1D(encoding_dim, kernel_size=3, activation='relu', padding='same')(encoded)\n",
    "    encoded = Flatten()(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = Reshape((1, np.prod(input_shape)//encoding_dim))(encoded)\n",
    "    decoded = Conv1DTranspose(32, kernel_size=3, activation='relu', padding='same')(decoded)\n",
    "    decoded = Conv1DTranspose(64, kernel_size=3, activation='relu', padding='same')(decoded)\n",
    "    decoded = Conv1DTranspose(128, kernel_size=3, activation='relu', padding='same')(decoded)\n",
    "    decoded = Conv1DTranspose(1, kernel_size=3, activation='sigmoid', padding='same')(decoded)\n",
    "    decoded = Reshape(input_shape)(decoded)\n",
    "\n",
    "    # Autoencoder\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    \n",
    "    # Encoder\n",
    "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d76c1685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15/15 [==============================] - 1s 24ms/step - loss: 0.0665\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0378\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0177\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0098\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.0065\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.0053\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0042\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.0034\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0026\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0022\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.0020\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.0018\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.0017\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0016\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.0016\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.0015\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.0014\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0014\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0013\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.0012\n",
      "Epoch 21/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0011\n",
      "Epoch 22/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.0010\n",
      "Epoch 23/30\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 9.0635e-04\n",
      "Epoch 24/30\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 8.1128e-04\n",
      "Epoch 25/30\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 7.4172e-04\n",
      "Epoch 26/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 6.7944e-04\n",
      "Epoch 27/30\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 6.3868e-04\n",
      "Epoch 28/30\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 6.2561e-04\n",
      "Epoch 29/30\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 5.9747e-04\n",
      "Epoch 30/30\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 5.6939e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2edc195a0>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 데이터의 형태\n",
    "input_shape = normalized_data.shape[1:]\n",
    "encoding_dim = 64\n",
    "\n",
    "# 가우시안 노이즈 추가\n",
    "noise_factor = 0.5\n",
    "noisy_data = add_gaussian_noise(normalized_data, noise_factor)\n",
    "\n",
    "# Denoising Autoencoder 모델 생성\n",
    "autoencoder, encoder = create_denoising_autoencoder(input_shape, encoding_dim)\n",
    "\n",
    "# 모델 학습\n",
    "autoencoder.fit(noisy_data, normalized_data, epochs=30, batch_size=16, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6bb153",
   "metadata": {},
   "source": [
    "# feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5e38b438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "# 특징 추출 (인코더의 출력)\n",
    "eeg_train_features = encoder.predict(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "99ed9df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(234, 64)\n"
     ]
    }
   ],
   "source": [
    "print(eeg_train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ee643f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.6729355, 12.994658 ,  0.       , ...,  0.       ,  0.       ,\n",
       "         9.369478 ],\n",
       "       [11.948198 , 12.608524 ,  0.       , ...,  0.       ,  0.       ,\n",
       "        10.511625 ],\n",
       "       [11.859114 , 12.108788 ,  0.       , ...,  0.       ,  0.       ,\n",
       "        10.715914 ],\n",
       "       ...,\n",
       "       [10.999057 ,  7.7388725,  0.       , ...,  0.       ,  0.       ,\n",
       "        11.660636 ],\n",
       "       [ 9.300373 ,  1.5316507,  0.       , ...,  0.       ,  3.590412 ,\n",
       "        10.831481 ],\n",
       "       [11.186989 , 14.51754  ,  0.       , ...,  0.       ,  0.       ,\n",
       "         4.5714107]], dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2fb63ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#특징 변환 및 저장\n",
    "df_encoded = pd.DataFrame(eeg_train_features)\n",
    "output_file = \"eeg_train_features.csv\"\n",
    "df_encoded.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
